allows for playlists to have "sources". This means that a playlist will update upon additions to any of its sources, but if other songs are added to a source and then deleted from the destination they will not be copied.

ex: 1 : {a,b} , 2 : {d, e} 3 : dest
3 will initially contain: {a,b,d,e}
then user deletes song a from 3
3:{b,d,e}
user adds c to 1:
1: {a,b,c} 3 : {b,c,d,e}
user deletes d from 2
2: {e} (no change to 3)

frontend to backend is plain HTTP requests with JSON data 

Backend endpoints: auth, get and show all playlists, show and create drainlists

daily run the sync. 

currently the whole thing works but its rather messy. 


Frontend endpoints are: 
1. add / remove source (local file changes only)
2. create / list drains (create is immediately followed by a populate, list is locals only )
3. list playlists (API call to get all playlists)

points of improvement: responsiveness, rn the lists dont change and interactivity is pretty bad

3. cant really be changed since we dont want the frontend to have rights to Spotify (messy)

the apis themselves are fine, wrapping around them can be done to make things prettier but at the end of the day the data that needs to make it across is working, and no unneccesary API calls are made

Backend:
1. everything is files (lots of manangemet with "open(a + b + c + d)" tons of places to go wrong)
2. temp playlists are written out to disk (needless)

Dont do ANY memory deduplication if two sourced playlists use the same source make 2 copies, removes race conditions (this can be changed later, but 5k of text is teeny tiny)

change to python shelve, pickle everything, Persistent dictionary
structure:
dict:
    secrets
    user
        tokens
            access
            refresh
            TTL
        plists
            p1 (name is from current user, ref is always 0 for DB loaded plists)
                name
                uri
                trakcs
            p2
                name
                uri
                trakcs
    user 2
        tokens
            access
            refresh
            TTL
        plists
            p1
                name
                uri
                trakcs
            p2
                name
                uri
                trakcs
.... etc ....


Lambda does memory in 100M chunks, use this and if it every gets too big (20k+ sources total) then you can put it in Dynamo or something

This removes the whole concept of writing a reference out to disk so now we can do comparisons in memory

sync, compare objects, update reference object, sync changes, report Diff to be used elsewhere. Syncing / closing ensures the memory dict and the file dict are the same 

with tokens in DB the token code will slightly clean up, but a lot of the gross url stuff has to stay, perhaps into its own file later (now just keep it at the bottom)

the rest of SPIO wont really change that much

playlist will lose writeouts and open playist (thank god) and the from file constructor
